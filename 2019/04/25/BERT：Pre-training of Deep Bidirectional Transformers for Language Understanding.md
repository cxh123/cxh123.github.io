
# BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding

## 一、介绍
本文介绍一种称之为BERT的新语言表征模型，意为来自变换器的双向编码器表征量(BidirectionalEncoder Representations from Transformers)。不同于最近的语言表征模型(Peters等，2018; Radford等，2018)，BERT旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的BERT表征可以仅用一个额外的输出层进行微调，进而为很多任务(如问答和语言推理)创建当前最优模型，无需对任务特定架构做出大量修改。

BERT的概念很简单，但实验效果很强大。它刷新了11个NLP任务的当前最优结果，包括将GLUE基准提升至80.4%(7.6%的绝对改进)、将MultiNLI的准确率提高到86.7%(5.6%的绝对改进)，以及将SQuADv1.1问答测试F1的得分提高至93.2分(1.5分绝对提高)——比人类性能还高出2.0分。

预训练语言表征：

1. 基于特征feature-based
2. 基于微调fine-tuning

### 基于特征
ELMo(Peters等，2017)将传统的词嵌入研究概括为不同维度。他们建议从语言模型中提取语境敏感型特征。把语境字词嵌入与现有任务特定架构集成时，ELMo针对一些主要的NLP基准(Peters et al., 2018)提出了最先进的技术，包括关于SQUAD问答(Rajpurkar等，2016)，情绪分析(Socher等，2013)，以及命名实体识别(Tjong Kim Sang和De Meulder，2003)。

### 基于微调
GenerativePre-trained Transformer

局限：单向

改进：BERT通过提出一个新的预训练目标：“遮蔽语言模型”(maskedlanguage model，MLM)，来解决前面提到的单向局限。

>该遮蔽语言模型随机地从输入中遮蔽一些词块，并且，目标是仅基于该遮蔽词语境语境来预测其原始词汇id。不像从左到右的语言模型预训练，该MLM目标允许表征融合左右两侧语境语境，这允许我们预训练一个深度双向变换器。除了该遮蔽语言模型，我们还引入了一个“下一句预测”(nextsentence prediction)任务，该任务联合预训练文本对表征量。

### 从监督数据转移学习

虽然无监督预训练的优势在于可获得的数据量几乎无限，但也有工作表明从具有大型数据集的监督任务中可有效迁移，例如自然语言推理(Conneau等，2017)和机器翻译(Mc-Cann等，2017)。在NLP之外，计算机视觉研究也证明了从大型预训练模型迁移学习的重要性，其中一个有效的方法是微调在ImageNet上预训练的模型(Deng等，2009; Yosinski等，2014)。

## 二、BERT变换器双向编码器表征
### 1. 模型架构Model Architecture
>BERT模型架构是一种多层双向变换器（Transformer）编码器，基于Vaswani等人(2017年)描述并在tensor2tensor库发行的原始实现。因为变换器的使用最近变得无处不在，我们架构的实施有效地等同于原始实现，所以我们会忽略模型架构详尽的背景描述，并向读者推荐Vaswani等人(2017)的优秀指南，如“注释变换器”。

　  在这项工作中，我们把层数(即Transformer blocks变换器块)表征为L，隐节点大小表征为H，自注意力数目表征为A。在所有情况下，我们设置前馈/过滤器的尺寸为4H，如H=768时为3072，H=1024时为4096。我们主要报告在两种模型尺寸上的结果：
　  

**$BERT_{BASE}$：L=12，H=768，A=12，总参数=110M**

**$BERT_{LARGE}$：L=24，H=1024，A=16，总参数=340M**
    
　　选择的$BERT_{BASE}$模型尺寸等同于OpenAIGPT模型尺寸，以进行比较。然而，重要的是，BERT变换器使用双向自注意，而GPT变换器使用受限自注意，每个词块只能注意其左侧语境。我们注意到，在文献中，双向变换器通常指称为“变换器编码器”，而其左侧语境版本被称为“变换器解码器”，因为它可用于文本生成。BERT，OpenAIGPT和ELMo之间的比较如图1所示。
　
![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181207115222802-1434326712.png)

图1：预训练模型架构间差异。BERT使用双向变换器，OpenAI GPT使用从左到右的变换器，ELMo使用独立训练的从左到右和从右到左LSTM级联来生成下游任务的特征。三种模型中只有BERT表征基于所有层左右两侧语境。

### 2. 输入表征
输入表征：能在一个词块序列中明确地表征单个文本句子或一对文本句子（[问题，答案]）。

>(注：在整个这项工作中，“句子”可以是连续文本的任意跨度，而不是实际的语言句子。“序列”指BERT的输入词块序列，其可以是单个句子或两个句子打包在一起。)对于给定词块，其输入表征通过对相应词块的词块嵌入、段嵌入和位嵌入求和来构造。图2给出了我们的输入表征的直观表征。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181213165937465-2014841944.png)

图2：BERT输入表征。输入嵌入是词块嵌入、段嵌入和位嵌入的总和。

具体是：

 - 使用WordPiece嵌入(Wu等，2016)和30,000个词块表。用##表征分词。
 - 使用学习的位置嵌入，支持的序列长度最多为512个词块。
 - 每个序列的第一个词块始终是特殊分类嵌入([CLS])。对应该词块的最终隐藏状态(即，变换器输出)被用作分类任务的聚合序列表征。对于非分类任务，将忽略此向量。
 - 句子对被打包成单个序列。我们以两种方式区分句子。首先，我们用特殊词块([SEP])将它们分开。其次，我们添加一个学习句子A嵌入到第一个句子的每个词块中，一个句子B嵌入到第二个句子的每个词块中。
 - 对于单句输入，我们只使用句子A嵌入。

### 3. 预训练任务

>与Peters等人(2018)和Radford等人(2018)不同，我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用两个新型无监督预测任务对BERT进行预训练，如本节所述。

### 任务1：遮蔽语言模型

直观地说，有理由相信深度双向模型比左向右模型或从左到右和右到左模型的浅层连接更严格。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，**因为双向调节将允许每个单词在多层语境中间接地“看到自己”。**

为了训练深度双向表征，我们采用一种直接方法，**随机遮蔽输入词块的某些部分，然后仅预测那些被遮蔽词块。**我们将这个过程称为“遮蔽LM”(MLM)，尽管它在文献中通常被称为Cloze完形任务(Taylor, 1953)。在这种情况下，对应于遮蔽词块的最终隐藏向量被馈送到词汇表上的输出softmax函数中，如在标准LM中那样预测所有词汇的概率。在我们所有实验中，我们随机地遮蔽蔽每个序列中所有WordPiece词块的15％。与去噪自动编码器(Vincent等，2008)相反，我们只预测遮蔽单词而不是重建整个输入。

> **缺点1：**
> 我们正在创建预训练和微调之间的不匹配，因为在微调期间从未看到[MASK]词块。
> 
> **解决办法：**
>为了缓解这个问题，我们并不总是用实际的[MASK]词块替换“遮蔽”单词。相反，训练数据生成器随机选择15％的词块，例如，在句子：*我的狗是毛茸茸的，它选择毛茸茸的。*然后完成以下过程：
> 并非始终用[MASK]替换所选单词，数据生成器将执行以下操作：

> * 80％的时间：用[MASK]词块替换单词，例如，我的狗是毛茸茸的！我的狗是[MASK]
> * 10％的时间：用随机词替换遮蔽词，例如，我的狗是毛茸茸的！我的狗是苹果
> * 10％的时间：保持单词不变，例如，我的狗是毛茸茸的！我的狗毛茸茸的。这样做的目的是将该表征偏向于实际观察到的单词。

变换器编码器不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入词块的分布式语境表征。此外，因为随机替换只发生在所有词块的1.5％(即15％的10％)，这似乎不会损害模型的语言理解能力。

> **缺点2:**
使用MLM的第二个缺点是每批中只预测了15％的词块，这表明模型可能需要更多的预训练步骤才能收敛。在5.3节中，我们证明MLM的收敛速度略慢于从左到右的模型(预测每个词块)，但MLM模型在实验上的改进远远超过所增加的训练成本。

### 任务2：下一句预测
>很多重要的下游任务，例如问答(QA)和自然语言推理(NLI)，都是基于对两个文本句子间关系的理解，而这种关系并非通过语言建模直接获得。

为了训练一个理解句子关系的模型，我们预训练了一个二值化下一句预测任务，该任务可以从任何单语语料库中轻松生成。具体来说，选择句子A和B作为预训练样本：B有50%的可能是A的下一句，也有50%的可能是来自语料库的随机句子。例如：

> 输入=[CLS]男子去[MASK]商店[SEP]他买了一加仑[MASK]牛奶[SEP]
> 
> Label= IsNext
> 
> 输入=[CLS]男人[MASK]到商店[SEP]企鹅[MASK]是飞行##少鸟[SEP]
> 
> Label= NotNext

我们完全随机选择这些NotNext语句，最终预训练模型在此任务中达到97％-98％的准确率。尽管它很简单，但我们在5.1节中证明，面向该任务的预训练对QA和NLI都非常有益。

### 4.预训练过程

>BERT预训练过程主要遵循现有的语言模型预训练文献。
>
>对于预训练语料库，我们使用BooksCorpus(800M单词)(Zhu等，2015)和英语维基百科(2,500M单词)的串联。对于维基百科，我们只提取文本段落并忽略列表、表格和题头。至关重要的是，使用文档级语料库而不是洗牌式(乱词序)句子级语料库，例如Billion Word Benchmark(Chelba等，2013)，以便提取长的连续序列。

为了生成每个训练输入序列，我们从语料库中采样两个文本跨度，我们将其称为“句子”，即使它们通常比单个句子长得多(但也可以更短)。第一个句子接收A嵌入，第二个句子接收B嵌入。B有50％可能刚好是A嵌入后的下一个句子，亦有50％可能是个随机句子，此乃为“下一句预测”任务而做。对它们采样，**使其组合长度≦512个词块**。该LM遮蔽应用于具有15％统一掩蔽率的WordPiece词块化之后，并且不特别考虑部分字块。

* 训练批量大小为256个序列(256个序列*512个词块=128,000个词块/批次)，
* 持续1,000,000个步骤，这比33亿个单词语料库大约40个周期。
* 我们使用Adam(学习程序)，设其学习率为1e-4，β1=0.9，β2=0.999，
* L2权重衰减为0.01，学习率预热超过前10,000步以上以及线性衰减该学习率。
* 我们在所有层上使用0.1的丢失概率。
* 在OpenAIGPT之后，我们使用gelu激活(Hendrycks和Gimpel, 2016)而不是标准relu。
* 训练损失是平均的遮蔽LM可能性和平均的下一句子预测可能性的总和。

>在Pod配置的4个云TPU上进行了B$BERT_{BASE}$训练(总共16个TPU芯片)。(注：https://cloudplatform.googleblog.com/2018/06/Cloud-TPU-now-offers-preemptible-pricing-and-globalavailability.html)在16个云TPU(总共64个TPU芯片)进行了$BERT_{LARGE}$训练。每次预训练需4天完成。

### 5.微调过程
>对于序列级分类任务，BERT微调很简单。


* 为了获得输入序列的固定维度池化表征，我们对该输入第一个词块采取最终隐藏状态(例如，该变换器输出)，通过对应于特殊[CLS]词嵌入来构造。将该向量表示为$C∈R^H$。
* 微调期间添加的唯一新参数是分类层向量$W∈R^{KxH}$，其中K是分类器标签的数量。
* 该标签概率$P∈R^K$用标准softmax函数，$P=softmax(CW^T)$计算。
* BERT和W的所有参数都经过联动地微调，以最大化正确标签的对数概率。
* 对于跨度级和词块级预测任务，必须以任务特定方式稍微修改上述过程。

详情见第4节的相应小节。

>对于微调，大多数模型超参数与预训练相同，但批量大小、学习率和训练周期数量除外。

* 丢失概率始终保持在0.1。最佳超参数值是特定于任务的，但我们发现以下范围的可能值可以在所有任务中很好地工作：
* 批量大小：16,32
* 学习率(Adam)：5e-5,3e-5,2e-5
* 周期数量：3,4

我们还观察到，大数据集(如100k+词块的训练样例)对超参数选择的敏感性远小于小数据集。微调通常非常快，因此需合理简单地对上述参数进行详尽搜索，并选择开发集上性能最佳的模型。

## 三、BERT和OpenAI GPT比较

>与BERT最具可比性的现有预训练方法是OpenAI GPT，它在大型文本语料库中训练左到右的变换器LM。
>
>实际上，许多BERT设计决策被有意地选择为尽可能接近GPT，以便最细微地比较这两种方法。这项工作的核心论点是占主要经验改进的3.3节中提出的两个新型预训练任务，但我们注意到BERT和GPT在如何训练之间还存在其他一些差异：

* GPT在BooksCorpus(800M单词)训练；BERT在BooksCorpus(800M单词)和维基百科(2,500M单词)训练。
* GPT使用一种句子分隔符([SEP])和分类符词块([CLS])，它们仅在微调时引入；BERT在预训练期间学习[SEP]，[CLS]和句子A/B嵌入。
* GPT用一个批量32,000单词训练1M步；BERT用一个批量128,000单词训练1M步。
* GPT对所有微调实验使用的5e-5相同学习率；BERT选择特定于任务的微调学习率，在开发集表现最佳。
　　

为了分离这些差异的影响，我们在5.1节进行了消融实验，证明大多数改进实际上来自新型预训练任务。

## 四、实验
###1.GLUE数据集
>通用语言理解评估(GLUE)基准(Wang等，2018)是各种自然语言理解任务的集合。

大多数GLUE数据集已存在多年，但GLUE的目的是：

1. 使用规范的Train、Dev和Test拆分发行这些数据集； 
2. 设置评估服务器以减轻评估不一致事件和测试集过度拟合。

GLUE不会为测试集分发标签，用户必须将其预测上传到GLUE服务器进行评估，并限制提交的数量。

GLUE基准包括以下数据集，其描述最初在Wang等人(2018)的文章中进行了总结：

* MNLI
多类型自然语言推理是一项大规模的众包蕴涵分类任务(Williams等，2018)。给定一对句子，目标是预测第二句与第一句相比是蕴涵、矛盾还是中立。
* QQP
Quora问题对是一个二元分类任务，其目的是确定Quora上提出的两个问题是否在语义上是等价的(Chen等，2018)。
* QNLI
问题自然语言推理是斯坦福问题答疑数据集(Rajpurkar等，2016)的一个版本，已被转换为二元分类任务(Wang等，2018)。正例是(问题，句子)对包含正确答案，而负例是(问题，句子)来自同一段落，不包含答案。
* SST-2
斯坦福情感树库2是一个二元单句分类任务，由从电影评论中提取的句子和人类注释的情绪组成(Socher等，2013)。
* CoLA
语言可接受性语料库是一个二元单句分类任务，其目标是预测英语句子在语言上是否“可接受”(Warstadt等，2018)。
* STS-B
语义文本相似性基准是从新闻标题和其他来源中提取的句子对的集合(Cer等，2017)。它们用1到5的分数进行注释，表示两个句子在语义上的相似程度。
* MRPC
微软研究院解释语料库由从在线新闻源自动提取的句子对组成，其中人类注释是否该对中的句子是否在语义上相等(Dolan和Brockett，2005)。
* RTE
识别文本蕴涵是类似于MNLI的二进制蕴涵任务，但训练数据少得多(Bentivogli等，2009)。(注：请注意，本文仅报告单任务微调结果。多任务微调方法可能会进一步推动结果。例如，我们确实观察到MNLI多任务培训对RTE的实质性改进。)
* WNLI
威诺格拉德自然语言推理是一个源自(Levesque等，2011)的小型自然语言推理数据集。GLUE网页指出，该数据集的构建存在问题，并且每个提交给GLUE训练过的系统的性能都比预测大多数类别的65.1基线准确度差。(注：https://gluebenchmark.com/faq) 

因此，我们将这一组排除在OpenAIGPT的公平性之外。对于我们的GLUE提交，我们总是预测其大多数的类。

### GLUE结果

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181214173656763-179180216.png)

图3：我们的任务特定模型是由向BERT添加一个额外输出层而形成的，因此一小部分参数需要从头开始学习。在该任务中，(a)和(b)是序列级任务，(c)和(d)是词块级任务。图中E代表其输入嵌入，Ti代表词块i的语境表征，[CLS]是分类输出的特殊符号，[SEP]是分割非连续词块序列的特殊符号。

* 对GLUE微调，我们呈现了第3节中描述的输入序列或序列对，并使用对应于第一个输入词块([CLS])的最终隐藏向量$C∈R^H$作为聚合表征。这都呈现在可视化图3(a)和(b)中。
* 在微调期间引入的唯一新参数是分类层$W∈R^{K×H}$，其中K是标签数量。
* 我们用C和W计算标准分类损失，即$log(softmax(CW^T))$。
* 对所有GLUE任务，我们均在其数据上使用一个批量大小为32和3个周期。
* 对于每项任务，我们用学习率5e-5,4e-5,3e-5和2e-5做了微调，并选择了在其Dev集上性能最佳的那一个。
* 此外，对于$BERT_{LARGE}$，我们发现微调有时在小数据集上不稳定(如，某些运行会产生退化结果)，因此我们运行了几次**随机重启**并选择了在Dev集上性能最佳的模型。
* 通过随机重启，我们使用相同的预训练检查点，但执行不同的微调数据混洗和分类器层初始化。
* 我们注意到GLUE数据集分布不包括其测试标签，我们只为每个$BERT_{BASE}$和$BERT_{LARGE}$做单一的GLUE评估服务器提交。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181214173907110-205952700.png)

表1：GLUE测试结果，评分来自其GLUE评估服务器。每个任务下面的数字代表该训练样本数量。“Average”列与GLUE官方分数略微不同，因为我们排除了有问题的WNLI集。

> OpenAI GPT = (L=12, H=768, A=12)； 
> 
> $BERT_{BASE}$= (L=12, H=768, A=12)；
> 
> $BERT_{LARGE}$ = (L=24, H=1024,A=16)。 
> 
> BERT和OpenAI GPT是单模型、单任务。所有结果来自于以下地址：https://gluebenchmark.com/leaderboard和https://blog.openai.com/language-unsupervised/。

结果如表1所示。$BERT_{BASE}$和$BERT_{LARGE}$在所有任务上的性能均优于所有现有系统，相对于最先进水平，平均准确度提高了4.4％和6.7％。请注意，$BERT_{BASE}$和OpenAIGPT在其注意遮蔽之外的模型架构几乎相同。对于规模最大、报道最广泛的GLUE任务，MNLI、BERT的绝对精度提高了4.7％，超过了最先进水平。在官方GLUE排行榜上，$BERT_{LARGE}$得分为80.4，而该排行榜系统登顶的OpenAIGPT在本文撰写之日获得72.8分。(注 https://gluebenchmark.com/leaderboard)

有趣的是，$BERT_{LARGE}$在所有任务中都明显优于$BERT_{BASE}$，即使训练数据非常少的那些也是如此。第5.2节更全面地探讨了BERT模型尺寸的影响。

### 2.斯坦福问答数据集 SQuAD v1.1
Standford问题回答数据集(SQuAD)是一种100k众包问答对的集合(Rajpurkar等，2016)。给出一个问题和包含答案的来自维基百科的一个段落，任务是预测该段落中的其答案文本的跨度。例如：

>**输入问题：**
>
> 水滴在哪里与冰晶碰撞形成沉淀？ 
> 
> **输入段落：**
> 
> ...沉淀形成为较小的液滴通过与云中的其他雨滴或冰晶碰撞而聚结。...
> 
> **输出答案：**
> 
> 在云中

这种类型的跨度预测任务与GLUE的序列分类任务完全不同，但我们能以简单的方式调整BERT以在SQuAD上运行。与GLUE一样，我们将输入问题和段落表示为单个打包序列，问题使用A嵌入和使用B嵌入的段落。在微调期间学习的唯一新参数是起始矢量$S∈R^H$和结束矢量$E∈R^H$。让来自BERT的第i个输入词块的最终隐藏向量表示为$T_i∈R^H$。请参见可视化图3(c)。然后，单词 i 作为答案跨度开始的概率被计算为$T_i$和$S$之间的点积(dot product)，跟随着段落中所有单词的softmax：

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220114949078-713910837.png)

相同公式用于其答案跨度的末端，最大评分范围用作其预测。训练目标是正确的开始和结束位置的log似然(log-likelihood)。

我们以学习率5e-5批量大小32来训练3个周期。推理时，由于结束预测不以开始为条件，我们添加了在开始后必须结束的约束，但是没有使用其他启发式方法。词块化标记跨度与原始非词块化输入对齐，以做评估。

结果呈现在表2。SQuAD用很严格的测试过程，其提交者必须人工联系SQuAD组织者以在一个隐藏测试集上运行他们的系统，因此我们只提交了我们最好的系统进行测试。该表显示的结果是我们向SQuAD提交的第一个也是唯一的测试。我们注意到SQuAD排行榜最好高结果没有最新的可用公共系统描述，并且在训练他们的系统时可以使用任何公共数据。因此，我们通过我们提交的系统中使用非常适度的数据增强，在SQuAD和TriviaQA(Joshi等，2017)上联合训练。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181214174559171-1400833149.png)

表2：SQuAD结果。本BERT集成是使用不同预训练检查点和微调种子(fine-tuning seed)的7x系统。

我们性能最佳的系统在整体排名中优于顶级排行榜系统+1.5 F1项，在单一系统中优于+1.3 F1项。事实上，我们的单一BERT模型在F1得分方面优于顶级全体系统。如果我们只微调SQuAD(没有TriviaQA)，我们将失去0.1-0.4的F1得分，但仍然大幅超越所有现有系统。

### 3.命名实体识别（Named Entity Recognition）
> 为了评估词块标记任务的性能，我们在CoNLL 2003命名实体识别(NER)数据集上微调BERT。该数据集由200k个训练单词组成，这些单词已注释为人员、组织、位置、杂项或其他(非命名实体)。

为做微调，我们将最终隐藏表征$T_i∈R^H$提供给每个词块i到NER标签集上的分类层。此预测不以周围预测为条件(即，非自回归和无CRF)。为了使其与WordPiece词块化相兼容，我们将每个CoNLL词块化输入单词提供给我们的WordPiece词块化器，并使用与第一个子标记相对应的隐藏状态作为分类器的输入。例如：
> Jim　　Hen　　##son　　was　　a　　puppet　　##eer
> 
> I-PER　　I-PER　　X　　O　　O　　O　　X

在没有对X做预测的情况下。由于WordPiece词块化边界是一个该输入的已知部分，因此对训练和测试都做了预测。图3(d)中还给出了可视化呈现。一种事例WordPiece模型用于NER，而非事例模型用于所有其他任务。

结果呈现在表3中。$BERT_{LARGE}$优于现有SOTA——具有多任务学习(Clark等，2018)的跨视图训练，在CoNLL-2003NER测试中达+0.2。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181214175151318-519298288.png)

表3：CoNLL-2003命名实体识别结果。超参数通过开发集来选择，得出的开发和测试分数是使用这些超参数进行五次随机重启的平均值。

### 4.对抗生成情境数据集（SWAG）
此对抗生成情境(SWAG)数据集包含113k个句子对的完成样例，用于评估基础常识推理(Zellers等，2018)。给定一个视频字幕数据集中的某一个句子，任务是在四个选项中决定最合理的后续。例如：

>一个女孩正穿过一套猴架杆。她
>
>(i) 跳过猴架杆。
>
>(ii) 挣扎到架杆抓住她的头。
>
>(iii) 走到尽头，站在木板上。
>
>(iv) 跳起并做后退。
>
>(译注：monkey bars n.猴架，供孩子们攀爬玩耍的架子)

调到SWAG数据集的BERT，类似于其GLUE适配。对于每个样本，我们构造四个输入序列，每个输入序列包含给定句子(句子A)和可能后续(句子B)的串联。我们引入的唯一任务特定参数是一个矢量$V∈R^H$，其具有最终聚合表征$C_i∈R^H$的点积代表每个选择i的得分。概率分布是四种选择的softmax：

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220115058877-1203308287.png)

我们用学习率2e-5批量大小16，对此模型做了3个周期的微调。结果呈现在表4。{BERT_LARGE}的性能优于该作者ESIM+ELMo系统的基线达+27.1％。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220115151741-396249415.png)

表4：SWAG开发和测试精度。测试结果由SWAG作者们对其隐藏标签进行评分。如SWAG论文所述，人类性能是用100个样本测量的。

## 五、消模实验

虽然我们已经演示了极其强大的实验结果，但到目前为止所呈现的结果并未分离BERT框架各个方面的具体贡献。在本节中，我们将对BERT多个方面进行消模实验，以便更好地了解它们的相对重要性。(译注：Quora上对ablation study的解释：An ablation study typicallyrefers to removing some “feature” of the model or algorithm, and seeing howthat affects performance.

**消模实验**通常是指删除模型或算法的某些“特征”，并查看如何影响性能。ablation study是为研究模型中提出的一些结构是否有效而设计的实验。比如你提出了某结构，但要想确定这个结构是否有利于最终效果，就要将去掉该结构的模型与加上该结构的模型所得到的结果进行对比。ablation study直译为“消融研究”，意译是“模型简化测试”或“消模实验”。)

### 1.预训练任务的影响
我们的核心主张之一是BERT的深度双向性，这是通过遮蔽LM预训练实现的，是BERT与以前工作相比最重要的改进。为证明这一主张，我们评估了两个使用完全相同预训练数据、微调方案和变换器超参数的BERTBASE新模型：

* 无NSP：一种使用“遮蔽LM”(MLM)训练但没有“下一句预测”(NSP)任务的模型。
* LTR＆NoNSP：使用从左到右(LTR)LM而不是MLM训练的模型。在这种情况下，我们预测每个输入单词，不应用任何遮蔽。左侧约束也用于微调，因为我们发现使用左侧语境预训练和双向语境微调，效果总是更差。此外，该模型在没有NSP任务的情况下做了预训练。这与OpenAIGPT直接相当，但使用我们更大的训练数据集、我们的输入表征和我们的微调方案。

结果显示在表5中。我们首先检查NSP任务带来的影响。我们可以看到，删除NSP会严重损害QNLI，MNLI和SQuAD的性能。这些结果表明，我们的预训练方法对于获得先前提出的强有力的实证结果至关重要。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220115652731-1683199029.png)

表5：用$BERT_{BASE}$架构做的预训练任务消融。“无NSP”是无下一句话预测任务的训练。“LTR＆无NSP”用作从左到右的LM，没有下一个句子预测，如OpenAI GPT的训练。“+ BiLSTM”在微调期间在“LTR +无NSP”模型上添加随机初始化BiLSTM。

接下来，我们通过比较“No NSP”与“LTR＆No NSP”来评估训练双向表征的影响。LTR模型在所有任务上的性能都比MLM模型差，在MRPC和SQuAD上有极大下降。对于SQuAD，直观清楚的是LTR模型在跨度和词块预测方面性能非常差，因为其词块级隐藏状态没有右侧语境。对于MRPC，目前尚不清楚性能不佳是由于其小数据量还是该任务本质，但我们发现这种不良性能在有很多随机重启的完整超参数扫描(full hyperparameter sweep)中是一致的。

为了诚心尝试加强该LTR系统，我们试着在其上面添加一个随机初始化BiLSTM做微调。这确实显着提升了SQuAD结果，但结果仍比预训练双向模型差得多。它还影响所有四个GLUE任务的性能。
我们认识到，也可以训练独立的LTR和RTL模型，并将每个词块表示为这两个模型的串联，如ELMo所做的那样。但是：

(a) 这是单一双向模型的两倍代价；

(b) 对于像QA这样的任务来说，这是不直观的，因为RTL模型无法对其问题的答案作出规定；

(c) 它的强度远低于深度双向模型，因为深度双向模型可以选择使用左或右语境。

### 2.模型大小的影响
>在本节，我们将探讨模型大小对微调任务准确性的影响。我们训练了许多具有不同层数、隐藏单元和注意头的BERT模型，与此同时，使用与前面描述的相同的超参数和训练过程。

选定GLUE任务的结果如表6所示。此表中，我们报告了5次随机重启微调的平均DevSet开发集精度。我们可以看到，较大的模型导致所有四个数据集的严格精度提高，即使对于仅有3,600个标记训练样例的MRPC，并且与预训练任务有很大不同。同样令人惊讶的是，我们能够在相对于现有文献已经相当大的模型之上实现这种显著改进。例如，Vaswani等人(2017)探索的其最大变换器，是(L=6，H=1024，A=16)有100M参数的编码器，我们在文献中找到的最大变换器是(L=64，H=512，A=2)有235M参数(Al-Rfou等，2018)。相比之下，$BERT_{BASE}$包含110M参数，$BERT_{LARGE}$包含340M参数。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220120029161-1444790981.png)

表6：BERT模型大小的消融。#L=层数; #H=隐藏的大小; #A=关注头数。“LM(ppl)”是保持训练数据的遮蔽LM混乱。

众所周知，增加模型尺寸将导致机器翻译和语言建模等大型任务的持续改进，这可通过表6中所示该LM训练数据的复杂性来证明。但是，我们相信这是第一个证明扩展到极端模型尺寸的工作也可以在非常小规模的任务上实现大幅改进，前提是该模型已经过充分预训练。

### 3.训练步数的影响
图4呈现了从已预训练k步的检查点进行微调后的MNLI Dev精度。这使我们可以回答以下问题：

> 1. 问题：BERT是否真的需要如此大量预训练(128,000字/批*1,000,000步)才能实现高微调精度？
> 答：是的，当训练1M步时，$BERT_{BASE}$在MNLI上实现了近1.0％的额外准确度，而步数为500k。
> 2. 问题：MLM预训练是否比LTR预训练收敛慢，因为每批只有15％的单词被预测而不是每个单词？
> 答：MLM模型的收敛速度略慢于LTR模型。然而，就绝对精度而言，MLM模型几乎立即开始优于LTR模型。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220120357985-401489932.png)

图4：多次训练步骤的消融。这显示了微调后的MNLI精度，从已经预训练了k步的模型参数开始。x轴是k的值。

### 4.基于特征的BERT方法
到目前为止呈现的所有BERT结果都使用了微调方法，其中将一个简单分类层添加到预训练模型，并且所有参数在下游任务上联合微调。然而，基于特征的方法具有某些优点，其固定特征从预训练模型中提取。首先，并非所有NLP任务都可以通过变换器编码器架构轻松表示，因此需要添加特定于任务的模型架构。其次，主要计算益处在于能够一旦预计算其训练数据的一个高开销表征，就在该表征顶部使用较少开销模型运行多次实验。

在本节中，我们通过在CoNLL-2003 NER任务上生成类似ELMo预训练的语境表征，来评估基于特征的方法中BERT性能如何。为此，我们用4.3节相同的输入表征，但用其来自一层或多层的激活，而不微调任何BERT参数。这些语境嵌入用作分类层之前随机初始化的双层768维BiLSTM作为输入。

结果显示在表7中。性能最佳的方法是连接来自预训练变换器其顶部四个隐藏层的词块表征，微调此整个模型后仅为0.3 F1。这表明BERT对于微调和基于特征的方法都是有效的。

![](https://img2018.cnblogs.com/blog/1192699/201812/1192699-20181220120835989-514484635.png)

表7：用BERT和CoNLL-2003 NER基于特征的方法消模。将来自此指定层的激活做组合，并馈送到双层BiLSTM中，而不向BERT反向传播。

## 六、结论
近期实验改进表明，使用迁移学习语言模型展示出的丰富、无监督预训练，是许多语言理解系统的集成部分。特别是，这些结果使得即使低资源任务，也能从很深的单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构，允许其相同的预训练模型去成功解决一系列广泛的NLP任务。

虽然实验结果很强，在某些情况下超过人类性能，但重要的未来工作是研究BERT能不能捕获其语言现象。

 


